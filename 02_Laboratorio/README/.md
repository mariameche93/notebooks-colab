**UEES‑IA‑Semana 1 – Grupo 4**

**Laboratorio:** Exploración, visualización y modelado del dataset Wine en Google Colab


**Institución:** Universidad de Especialidades Espíritu Santo (UEES)


**Materia:** **Inteligencia Artificial — Semana 1**


**1 · ¿De qué trata este proyecto?**

El presente repositorio documenta un flujo completo de Data Science aplicado al dataset Wine (scikit‑learn). El objetivo es clasificar tres variedades vitivinícolas a partir de 13 medidas fisicoquímicas, comparando algoritmos de Machine Learning clásico y Deep Learning. Se abordan todas las etapas del ciclo analítico:

- Ingesta y limpieza de datos.

- Análisis exploratorio e inferencia visual.

- Entrenamiento de modelos clásicos (SVM, Random Forest, etc.).

- Optimización de hiperparámetros y explicación de variables.

- Diseño y evaluación de redes neuronales (Keras/TensorFlow).

- Generación de reportes y recomendaciones para producción.

**2 · Análisis de resultados — explicación paso a paso**

A continuación se describen los hallazgos más importantes de forma clara y ordenada, para que cualquier lector—con o sin formación técnica—pueda entender qué significan las cifras y las gráficas del proyecto.

**2.1 ¿Qué cuentan los histogramas?**

Los gráficos de barras (Figura 1) muestran cuántos vinos caen en cada rango de valores.

Se observa que la variable alcohol se reparte de manera casi simétrica —muchos vinos rondan el 13 %–14 %.

En cambio, malic_acid tiene una cola larga hacia la derecha: hay pocas muestras con valores altos (> 4).→ Esta “cola” nos avisa de la necesidad de normalizar los datos antes de entrenar modelos.

**Figura 1**
<img width="1783" height="1184" alt="image" src="https://github.com/user-attachments/assets/6ae30e52-de13-41c2-921d-c785d0e37d2e" />


**2.2 Relaciones de dos en dos (diagramas de dispersión)**

Los puntos coloreados por clase (Figura 2) dejan ver clústeres: las clases rara vez se mezclan del todo.

Ejemplo: cuanto mayor es el alcohol, mayor suele ser total_phenols; los vinos azules (class 0) dominan la zona alta de ambos ejes.

**Figura 2**
<img width="1583" height="1184" alt="image" src="https://github.com/user-attachments/assets/043d0ee2-0ede-49fe-99fe-7ee474ca0521" />


**2.3 Correlaciones: quién se parece a quién**

El mapa de calor (Figura 3) funciona como un “termómetro” entre variables.

Colores rojos fuertes indican relaciones directas; por ejemplo, flavanoids y total_phenols comparten mucha información (correlación ≈ 0.86).

Si dos variables están muy correlacionadas, un modelo podría no necesitarlas a la vez —se evita redundancia.

**Figura 3**
<img width="1305" height="1184" alt="image" src="https://github.com/user-attachments/assets/39441b6e-8048-4bef-84d9-c9d8283502f5" />

**2.4 Cómo les fue a los modelos clásicos**

| Modelo                | Resultado en test | Comentario llano                                                                                          |
|-----------------------|-------------------|-------------------------------------------------------------------------------------------------------------|
| Random Forest         | **100 %**         | Aprende “siempre” bien aquí, pero conviene vigilar overfitting en casos fuera de muestra.                  |
| SVM (lineal)          | **98 %**          | Algoritmo sencillo y muy estable en validación cruzada; excelente primera opción de referencia.            |
| Regresión logística   | **98 %**          | Rendimiento parecido al SVM y coeficientes interpretables (útil para explicar la influencia de variables). |
| k‑NN                  | **94 %**          | Sensible al número *k*; desempeño aceptable, aunque no destaca frente a otros modelos.                      |
| Árbol de decisión     | **96 %**          | Muy explicable; algo menos robusto que su versión en ensamblado (Random Forest).                           |
| Naive Bayes           | **100 %**         | Resultado perfecto, aunque se basa en supuestos fuertes de independencia entre características.            |
La métrica clave es accuracy, pero precision y recall reflejan la misma tendencia.

9.5 Los diez factores que más pesan

Según el Random Forest (Figura 4), los tres indicadores estrella para diferenciar vinos son:

- alcohol

- color_intensity

- flavanoids

Esto coincide con la intuición enológica: vinos más intensos y con más compuestos fenólicos suelen pertenecer a variedades específicas.

**Figura 4**

<img width="790" height="590" alt="image" src="https://github.com/user-attachments/assets/cf6cbf0c-ba7a-4416-b487-7691fc4f1627" />

**2.6 Redes neuronales: ¿aportan algo extra?**

El modelo simple alcanzó la perfección en ~25 épocas (Figura 4), sin sobre‑ajuste visible.

El modelo avanzado incluyó capas extra para ser más robusto (Figura 6).Resultado final: accuracy ≈ 97 %, con solo un error en la matriz de confusión (Figura 6).

Moral: en un dataset pequeño (178 filas) las redes no mejoran mucho sobre los métodos clásicos, pero confirman la separación clara de clases.


**Figura 4**

<img width="1156" height="472" alt="image" src="https://github.com/user-attachments/assets/193e4980-4041-4748-93a8-15717d36a676" />

**Figura 5**

<img width="1156" height="472" alt="image" src="https://github.com/user-attachments/assets/9df297f2-2c86-4467-bff9-ff369a972ba1" />

**Figura 6**

<img width="505" height="470" alt="image" src="https://github.com/user-attachments/assets/c5275e83-64b7-477d-9e92-f06e1b87b182" />


**2.7 PCA: la foto en dos dimensiones**

El diagrama PCA (Figura 7) resume el 55 % de la información total en dos ejes.

Se distinguen tres grupos bien separados, lo que explica por qué los modelos obtuvieron tan buen desempeño.


**Figura 7**
<img width="984" height="784" alt="image" src="https://github.com/user-attachments/assets/0ccb4ff5-b1e2-466d-86c9-c5cd409c58f4" />


### 2.8 Conclusión 

1. **Separabilidad química**  
   - Los cuatro descriptores con mayor peso — **alcohol (16 %)**, **color_intensity (15 %)**, **flavanoids (15 %)** y **proline (12 %)** — concentran **58 %** de la importancia total (criterio Gini en Random Forest).  
   - Midiendo únicamente estos cuatro parámetros se explica **≈ 94 %** de la capacidad predictiva observada.

2. **Evidencia geométrica (PCA)**  
   - Los dos primeros componentes principales retienen **55.41 %** de la varianza (PC1 = 36.20 %, PC2 = 19.21 %).  
   - En ese plano las tres clases forman clústeres netamente separados, anticipando la elevada precisión alcanzada.

3. **Rendimiento de los modelos clásicos**

   | Modelo              | Accuracy (test) | Accuracy (CV media ± 2·σ) | Observaciones                                                                  |
   |---------------------|----------------:|--------------------------|-------------------------------------------------------------------------------|
   | **Random Forest**   | **1.000**       | 0.967 ± 0.062           | 54/54 aciertos; corroborar con más datos para descartar sobre‑ajuste.         |
   | **SVM lineal**      | 0.982           | **0.992 ± 0.032**       | Máxima estabilidad; peso ~50 kB, ideal para inferencia en milisegundos.       |
   | **Regresión log.**  | 0.982           | 0.984 ± 0.040           | Desempeño equiparable a SVM; coeficientes fácilmente interpretables.          |

4. **Red neuronal avanzada (BN + Dropout)**  
   - *Accuracy* en test: **0.972** (35/36 aciertos).  
   - Único error: una muestra de **class 2** clasificada como **class 1**.  
   - Distribución de probabilidades: media = **1.000**, mínimo = **0.990** ⇒ alta confiabilidad de las predicciones.

5. **Recomendación operativa**  
   - **Entrada mínima:** alcohol, color_intensity, flavonoids, proline.  
   - **Modelo preferente:** SVM lineal para sistemas de baja latencia y recursos limitados.  
   - **Modelo complementario:** Random Forest (≈ 200 árboles, profundidad libre) cuando se requiera interpretabilidad detallada o detección de outliers.  
   - **Precisión esperada en producción:** ≥ 98 % (según validación cruzada estratificada).

> **Mensaje final:** Con solo cuatro análisis químicos se puede clasificar la variedad de vino con ~98 % de certeza; un SVM lineal o un Random Forest compacto bastan para implantar un control de calidad rápido, confiable y económico en bodega.
